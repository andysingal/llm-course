{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial configuration of SAP AI Core for BYOM-OSS-LLM-AI-CORE\n",
    "This notebook automates the initial configurations for application BYOM-OSS-LLM-AI-CORE to bring open-sourced llms into SAP AI Core. Alternatively, you can perform the same with SAP AI Launchpad.\n",
    "- Review and update the configuration in config.json\n",
    "- Initialize a client of SAP AI Core SDK\n",
    "- Create a resource group\n",
    "- Register a docker secret\n",
    "- Prepare a dummy model placeholder in object store\n",
    "- Register Object Store Secret\n",
    "- Onboarding Git Repository and Create an Application for BYOM-OSS-LLM-AI-CORE\n",
    "- Synchronize the application and check its status\n",
    "- Create the configurations for scenarios ollama, local-ai, llama.cpp, vllm and infinity(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Copy [config.template.json](config.template.jso) as [config.json](config.json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cp config.template.json config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Review and Update configuration in [config.json](config.json)\n",
    "Please read the **comments** carefully in [config.json](config.json) and update the necessary configurations.  \n",
    "- **name**: used as name of git repository and application. \n",
    "- **resource_group**: \"default\" will be used if not specified. It is optional but recommended to create a dedicate resource group, and update it [config.json](config.json). By default, \"default\" resource group is in place for all the AI Core instances.AI Core with tree tier plan is not able to create a new resource group.\n",
    "- **ai_core_sk**: update with your own AI Core Service Key\n",
    "- **docker_secret**: Update the user and password etc in .dockerconfigjson. Please refer to [this document](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/register-your-docker-registry-secret) for more detail.\n",
    "    - username: Replace <REPLACE_WITH_YOUR_DOCKER_USERNAME> with your docker user name. \n",
    "    - password: Replace <REPLACE_WITH_YOUR_DOCKER_ACCESS_TOKEN> with your docker access token.\n",
    "- **git_repo**: update the git repo configuration with your owns\n",
    "    - repo_url: url to your forked repository. It should be: https://github.com/<YOUR_GITHUB_ORG_OR_USER>/btp-generative-ai-hub-use-cases\n",
    "    - user: Update with your github user\n",
    "    - access_token: Update with your github user access token\n",
    "- **object_store_secret**(Optional): Only needed by [llama.cpp](llama.cpp) and vllm(vllm) to host custom models(fine-tuning etc). Update with your object store secret info. The sample below only refers to register the object store secret for AWS S3, there are other object store supported in SAP AI Core. Please refer to [this document](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/register-your-object-store-secret) for more detail and adapt the object_store_secret section in config.json sample code. The object store secret is only required for self-hosting your private custom or fine-tuning models physically. \n",
    "    - name: the name of object store secret, default as object-store-secret. \n",
    "    - type: S3, OSS etc.\n",
    "    - bucket: the AWS S3 bucket used as your private llm repository for hosting fine-tuning models or a dummy model placeholder for model artifacts for public open-source llms to be used with SAP Generative AI Hub SDK.\n",
    "    - endpoint: AWS S3 endpoint. Default as s3.amazonaws.com\n",
    "    - region: The target s3 region for the bucket. Optional. Default as blank.\n",
    "    - pathPrefix: The path prefix after bucket name. Optional. Default as blank.\n",
    "    - AWS_ACCESS_KEY_ID: Replace with your own aws access key id\n",
    "    - AWS_SECRET_ACCESS_KEY: Replace with your own aws secret access key.\n",
    "- **model_placeholder**(Optional): Only needed by llama.cpp and vllm for hosting custom models(fine-tuning etc).The dummy model placeholder is used to create the s3 bucket and path_prefix folder in step 6, which is used to register object store secret in step 7. It is recommended to use the default values.\n",
    "    - model_file_name: default as \"dummy.txt\", which will be created dynamically in the sample code and as a placeholder for the model for artifact registration only.\n",
    "    - path: default as \"Dummy\". It is used in the url below.\n",
    "    - url: the url of the dummy model. Default as \"ai://object-store-secret/Dummy\". Please refer to [this document](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/artifacts) for more detail \n",
    "- **application**: The SAP AI Core application hosts the scenarios of ollama etc to serving open sourced llms in SAP AI Core\n",
    "    - path_in_repo: relative path to the serving templates. No change needed.\n",
    "- **configurations**: Review the configurations of the scenarios. By default, it is configured to load the mistral-7b quantization model with [resource plan infer.s in SAP AI Core](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/choose-resource-plan-c58d4e584a5b40a2992265beb9b6be3c) defined in [../byom-oss-llm-templates](../byom-oss-llm-templates). It is recommend to go ahead first with the default configurations in config.json.\n",
    "    - **Ollama**: By default, **llama3:latest** model is configured. Pull the model dynamically in [ollama/ollama.ipynb](ollama/ollama.ipynb)\n",
    "    - **LocalAI**: LocalAI allows you to [preload model during startup](https://localai.io/advanced/#preloading-models-during-startup). The initial configuration in config.json will preload model [Mistral-7B-OpenOrca-GGUF](https://github.com/go-skynet/model-gallery/blob/main/mistral.yaml) with local-ai on resource plan 'infer.s' defined in [local-ai-template.yaml](../byom-oss-llm-templates/local-ai-template.yaml). In its model config file, GPU acceleration isn't enabled, hence it is quite slow. To have GPU acceleration for a model, you may set in its model config yaml file. For example [mixtral-Q6.yaml](https://github.com/go-skynet/model-gallery/blob/main/mixtral-Q6.yaml). Please review the [full config model file reference](https://localai.io/advanced/#full-config-model-file-reference)\n",
    "        ```sh\n",
    "        f16: true \n",
    "        mmap: true \n",
    "        gpu_layers: xx \n",
    "        ```\n",
    "        In addition, you can install more models through end point /model/apply in [local-ai/local-ai.ipynb](local-ai/local-ai.ipynb). Please refer to https://localai.io/advanced/#preloading-models-during-startup\n",
    "    - **llama.cpp**: By default, model mistral-7b-instruct-v0.2.Q5_K_M.gguf is configured with alias as mistral. Unlike ollama and local-ai, llama.cpp scenario only supports one model in one configuration. If you need multiple models to be served with llama.cpp, please create multiple configurations through SAP AI Launchpad\n",
    "    - **vllm**: By default, model TheBloke/Mistral-7B-Instruct-v0.2-AWQ is configured. Unlike ollama and local-ai, vllm scenario only supports one model in one configuration. If you need multiple model to be served with llama.cpp, please create multiple configurations through SAP AI Launchpad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: Load the configurations from [config.json](config.json)\n",
    "The service key of AI Core are located in section ai_core_sk of [config.json](config.json).<br/>\n",
    "Please update it with your own service key before running this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations loaded from config.json\n",
      "name:  byom-open-source-llms resource_group:  oss-llm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initializations\n",
    "resource_group = config.get(\"resource_group\", \"default\")\n",
    "name = config.get(\"name\", \"open-source-llms\")\n",
    "print(\"Configurations loaded from config.json\")\n",
    "print(\"name: \", name, \"resource_group: \", resource_group )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4: Initialize AI Core SDK Client\n",
    "The service key of AI Core are located in section ai_core_sk of [config.json](config.json).<br/>\n",
    "Please update it with your own service key before running this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource group: oss-llm, name: byom-open-source-llms\n"
     ]
    }
   ],
   "source": [
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "\n",
    "ai_core_sk = config[\"ai_core_service_key\"]\n",
    "client = AICoreV2Client(base_url=ai_core_sk.get(\"serviceurls\").get(\"AI_API_URL\")+\"/v2\",\n",
    "                        auth_url=ai_core_sk.get(\"url\")+\"/oauth/token\",\n",
    "                        client_id=ai_core_sk.get(\"clientid\"),\n",
    "                        client_secret=ai_core_sk.get(\"clientsecret\"),\n",
    "                        resource_group=resource_group)\n",
    "print(f\"resource group: {resource_group}, name: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5: Create a dedicated resource group (Optional but recommended)\n",
    "resource_group defined here must be matched with resource_group in [config.json](config.json). Default as \"oss-llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resource_group_id': 'oss-llm-2', 'labels': None, 'status': None, 'created_at': None}\n"
     ]
    }
   ],
   "source": [
    "response = client.resource_groups.create(resource_group_id = resource_group)\n",
    "print(response.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Register Docker Secret within SAP AI Core\n",
    "\n",
    "Please skip this step if you have already registered your docker secret within SAP AI Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'secret has been created'}\n",
      "Docker Registry Secret: docker-secret\n"
     ]
    }
   ],
   "source": [
    "docker_secret = config[\"docker_secret\"]\n",
    "response = client.docker_registry_secrets.create(\n",
    "    name = docker_secret[\"name\"],\n",
    "    data = docker_secret[\"data\"]\n",
    ")\n",
    "\n",
    "print(response.__dict__)\n",
    "print(f'Docker Registry Secret: {docker_secret[\"name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: (Optional) Prepare a dummy model placeholder in object store\n",
    "Only needed by llama.cpp and vllm for hosting custom models(fine tuning etc) in SAP AI Core.<br/>\n",
    "In the sample code, we register object store secret for AWS S3. Please assure the sections of object_store_secret and model_placeholder have been setup in config.json. If not, please refer to step 2: Review and Update configuration in config.json. The following actions will be performed.\n",
    "- Create the s3 bucket\n",
    "- Upload the model file place holder to the s3 bucket with key as (s3://bucket/path_prefix/path), which will be used to create the model artifact of open-source llm for model identification with deployment_id and model_name in SAP Generative AI Hub SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract necessary details from the config\n",
    "object_store_secret = config[\"object_store_secret\"]\n",
    "object_store_secret_name = object_store_secret[\"name\"]\n",
    "bucket_name = object_store_secret[\"bucket\"]\n",
    "path_prefix = object_store_secret[\"pathPrefix\"]\n",
    "endpoint_url = object_store_secret[\"endpoint\"]\n",
    "region_name = object_store_secret.get(\"region\")  # region is optional\n",
    "aws_access_key_id = object_store_secret[\"data\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = object_store_secret[\"data\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "\n",
    "model_placeholder = config[\"model_placeholder\"]\n",
    "model_file_name = model_placeholder[\"model_file_name\"]\n",
    "path = model_placeholder[\"path\"]\n",
    "url = model_placeholder[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'open-llm-repo' already owned by you.\n",
      "File 'dummy.txt' uploaded to 'open-llm-repo/Dummy/dummy.txt'.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    #endpoint_url=endpoint_url,\n",
    "    #region_name=region_name,\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "# Create the S3 bucket\n",
    "try:\n",
    "    if region_name:\n",
    "        s3_client.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': region_name}\n",
    "        )\n",
    "    else:\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "except s3_client.exceptions.BucketAlreadyExists:\n",
    "    print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "except s3_client.exceptions.BucketAlreadyOwnedByYou:\n",
    "    print(f\"Bucket '{bucket_name}' already owned by you.\")\n",
    "\n",
    "# Create a dummy file to upload\n",
    "# model_file_name = 'dummy.txt'\n",
    "with open(model_file_name, 'w') as f:\n",
    "    f.write(\"This is a dummy file.\")\n",
    "\n",
    "# Define the key (path in the bucket) for the dummy file\n",
    "object_key = f\"{path_prefix}{path}/{model_file_name}\"\n",
    "\n",
    "# Upload the dummy file to create the folder structure for path_prefix\n",
    "s3_client.upload_file(model_file_name, bucket_name, object_key)\n",
    "print(f\"File '{model_file_name}' uploaded to '{bucket_name}/{object_key}'.\")\n",
    "\n",
    "# Clean up the local dummy file if desired\n",
    "os.remove(model_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: (Optional) Register Object Store Secret\n",
    "The object store will be only required for hosting your own **fine-tuning** llm models. As the public open-source llms and embedding models in our sample are hosted in public model hub like huggingface, ollama model library etc., and will be dynamically downloaded into its deployment in SAP AI Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'secret has been created'}\n"
     ]
    }
   ],
   "source": [
    "# Create object Store secret,\n",
    "# object_store_secret = config[\"object_store_secret\"]\n",
    "response = client.object_store_secrets.create(\n",
    "    name = object_store_secret[\"name\"], # identifier for this secret within your SAP AI Core\",\n",
    "    path_prefix = path_prefix, # path that we want to limit restrict this secret access to\",\n",
    "    type = object_store_secret[\"type\"],\n",
    "    bucket = bucket_name, \n",
    "    #region = \"eu-central-1\",  # optional\n",
    "    endpoint = endpoint_url, \n",
    "    data = object_store_secret[\"data\"]\n",
    "    # data = { \n",
    "    # AWS_ACCESS_KEY_ID\": object_store_secret[\"data\"][\"AWS_ACCESS_KEY_ID\"],\n",
    "    # AWS_SECRET_ACCESS_KEY\": object_store_secret[\"data\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    # }\n",
    ")\n",
    "\n",
    "print(response.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9: Update the serving templates\n",
    "Please replace the place holders in the following serving templates.\n",
    "- <YOUR_DOCKER_SECRET> to be replaced with **docker-secret** created in step 5 or your own docker secret.\n",
    "- <YOUR_DOCKER_USER> to be replaced with your own docker hub user.\n",
    "- ai.sap.com/resourcePlan: By default, the resource plan is as **infer.s**, which is sufficient for 7B model in the sample tests notebooks afterwards. If you would like to run 13B or 30B beyond etc, please use **infer.m** or **infer.l** resource plan. Check out more detail about [Resource Plan in SAP AI Core](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/choose-resource-plan-c58d4e584a5b40a2992265beb9b6be3c).\n",
    "```yaml\n",
    "    metadata:\n",
    "      #...\n",
    "      labels: |\n",
    "        ai.sap.com/resourcePlan: infer.s\n",
    "    spec: |\n",
    "      predictor:\n",
    "        imagePullSecrets:\n",
    "          - name: <YOUR_DOCKER_SECRET>\n",
    "          ...\n",
    "        containers:\n",
    "            - name: kserve-container\n",
    "              image: docker.io/<YOUR_DOCKER_USER>/ollama:ai-core\n",
    "```\n",
    "- [../byom-oss-llm-templates/llama.cpp-template.yaml](../byom-oss-llm-templates/llama.cpp-template.yaml)\n",
    "- [../byom-oss-llm-templates/local-ai-template.yaml](../byom-oss-llm-templates/local-ai-template.yaml)\n",
    "- [../byom-oss-llm-templates/ollama-template.yaml](../byom-oss-llm-templates/ollama-template.yaml)\n",
    "- [../byom-oss-llm-templates/vllm-template.yaml](../byom-oss-llm-templates/vllm-template.yaml)\n",
    "- [../byom-oss-llm-templates/infinity-template.yaml](../byom-oss-llm-templates/infinity-template.yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10: Onboard github repository and create an application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Repository has been on-boarded.\n",
      "Id: byom-open-source-llms, Message: Application has been successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Onboard repository\n",
    "repo_config = config[\"git_repo\"]\n",
    "repository = client.repositories.create(name,\n",
    "                                        url=repo_config.get(\"repo_url\"),\n",
    "                                        username=repo_config.get(\"user\"),\n",
    "                                        password=repo_config.get(\"access_token\")\n",
    "                                        )\n",
    "print(repository)\n",
    "\n",
    "# Create application\n",
    "app_config = config[\"application\"]\n",
    "application = client.applications.create(revision=app_config.get(\"revision\", \"HEAD\"),\n",
    "                                        path=app_config.get(\"path_in_repo\"),\n",
    "                                        application_name=name,\n",
    "                                        repository_name=name\n",
    "                                        )\n",
    "print(application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11: Check if application has synced and scenario created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health Status: Healthy, Sync Status: Synced, Sync Finished at: 2024-05-09T08:36:31Z\n",
      "Application synced\n",
      "Scenario {'name': 'ollama', 'id': 'byom-ollama-server'} synced\n",
      "Scenario {'name': 'local-ai', 'id': 'byom-local-ai-server'} synced\n",
      "Scenario {'name': 'llama.cpp', 'id': 'byom-llama.cpp-server'} synced\n",
      "Scenario {'name': 'vllm', 'id': 'byom-vllm-server'} synced\n",
      "Scenario {'name': 'infinity', 'id': 'byom-infinity-server'} synced\n"
     ]
    }
   ],
   "source": [
    "max_tries = 10\n",
    "i = 0\n",
    "interval_s = 20\n",
    "while i < max_tries:\n",
    "    i = i +1\n",
    "    app_status = client.applications.get_status(name)\n",
    "    print(f\"Health Status: {app_status.health_status}, Sync Status: {app_status.sync_status}, Sync Finished at: {app_status.sync_finished_at}\" )\n",
    "    \n",
    "    if(app_status.sync_status == \"Synced\"):\n",
    "        break\n",
    "\n",
    "    # Synchronize the application and wait\n",
    "    client.applications.refresh(name) \n",
    "    sleep(interval_s)\n",
    "\n",
    "if app_status.sync_status == \"Synced\":\n",
    "    print(\"Application synced\")\n",
    "    # Check scenarios\n",
    "    scenarios = client.scenario.query()\n",
    "\n",
    "    scenario_list = config[\"scenarios\"]\n",
    "    for scenario in scenario_list:\n",
    "        scenario_name = scenario[\"name\"]\n",
    "        scenario_exists = scenario_name in [s.name for s in scenarios.resources]\n",
    "        print(f\"Scenario {scenario} synced\") if scenario_exists else print(f\"Scenario {scenario_name} not yet available\")\n",
    "\n",
    "else:\n",
    "    #print(f\"Application not yet synced after 10 time retry. Likely, something wrong in the templates under git repo {repository.url}/{app_config.get(\"path_in_repo\")}.\\nPlease check it. You can run this cell again once it is fixed.\")\n",
    "    print(f\"Application not yet synced after 10 time retry. Please execute this cell again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12: Create configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json_file(file_path, key, value):\n",
    "    # Load the JSON configuration file\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    # Update the value\n",
    "    config[key] = value\n",
    "\n",
    "    # Write the updated configuration back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(config, file, indent=4)\n",
    "        print(f\"{file_path} updated. {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------byom-ollama-server--------------\n",
      "{'id': 'afb4c65a-be36-4bf4-adad-94a3ccf466d8', 'message': 'Artifact acknowledged', 'url': 'ai://object-store-secret/Dummy'}\n",
      "Id: 9338f418-36be-40c9-b739-5a12ec4f853e, Message: Configuration created\n",
      "ollama/env.json updated. configuration_id: 9338f418-36be-40c9-b739-5a12ec4f853e\n",
      "--------------byom-local-ai-server--------------\n",
      "{'id': '1849757a-cd5a-4bb3-9b3c-c5717fb727a9', 'message': 'Artifact acknowledged', 'url': 'ai://object-store-secret/Dummy'}\n",
      "Id: 05c671e4-5723-44ac-91e7-f769c8f1aaea, Message: Configuration created\n",
      "local-ai/env.json updated. configuration_id: 05c671e4-5723-44ac-91e7-f769c8f1aaea\n",
      "--------------byom-llama.cpp-server--------------\n",
      "{'id': 'e0be0462-0df5-48e2-a1b8-283c31db2b8c', 'message': 'Artifact acknowledged', 'url': 'ai://object-store-secret/Dummy'}\n",
      "Id: d93b2fa1-5c21-447c-ba3a-d39e9fcd35c8, Message: Configuration created\n",
      "llama.cpp/env.json updated. configuration_id: d93b2fa1-5c21-447c-ba3a-d39e9fcd35c8\n",
      "--------------byom-vllm-server--------------\n",
      "{'id': '7a0fe86f-e3ad-491b-9e04-e8a325f9970c', 'message': 'Artifact acknowledged', 'url': 'ai://object-store-secret/Dummy'}\n",
      "Id: 435f1738-c778-49c4-9323-66d23a138782, Message: Configuration created\n",
      "vllm/env.json updated. configuration_id: 435f1738-c778-49c4-9323-66d23a138782\n",
      "--------------byom-infinity-server--------------\n",
      "{'id': 'b08102d3-a2f4-48de-a666-f01b5f5eeb3a', 'message': 'Artifact acknowledged', 'url': 'ai://object-store-secret/Dummy'}\n",
      "Id: df6a72de-b72c-4c60-aad6-841998bacb29, Message: Configuration created\n",
      "infinity/env.json updated. configuration_id: df6a72de-b72c-4c60-aad6-841998bacb29\n",
      "config.json updated.\n"
     ]
    }
   ],
   "source": [
    "from ai_core_sdk.models import InputArtifactBinding,ParameterBinding\n",
    "from ai_api_client_sdk.models.artifact import Artifact\n",
    "from ai_api_client_sdk.models.label import Label\n",
    "\n",
    "# Create serving configurations\n",
    "conf_list = config[\"configurations\"]\n",
    "\n",
    "for conf in conf_list:\n",
    "    print(f'--------------{conf[\"scenario_id\"]}--------------')\n",
    "    # Create input artifacts for model\n",
    "    input_artifact_bindings = []\n",
    "    for ia in conf[\"input_artifacts\"]:\n",
    "        # Since it is a dummy model place holder, we only create the artifact when it is missing\n",
    "        # Otherwise, skip it.\n",
    "        if len(ia[\"artifact_id\"]) == 0:\n",
    "            artifact = client.artifact.create(\n",
    "                name = ia[\"name\"], # Custom Non-unqiue identifier\n",
    "                kind = Artifact.Kind.MODEL,\n",
    "                url = ia[\"url\"], \n",
    "                scenario_id = conf[\"scenario_id\"],\n",
    "                description = ia[\"description\"]\n",
    "                # labels = [\n",
    "                #      Label(key=\"ext.ai.sap.com/model\", value=ia[\"name\"]) # any descriptive key-value pair, helps in filtering, key must have the prefix ext.ai.sap.com/\n",
    "                # ]\n",
    "            )\n",
    "            \n",
    "            print(artifact.__dict__)\n",
    "            # Write back the artifact_id to configuration\n",
    "            ia[\"artifact_id\"] = artifact.id\n",
    "        input_artifact_bindings.append(InputArtifactBinding(key=ia['key'], artifact_id=ia[\"artifact_id\"]))\n",
    "    parameter_bindings = [ParameterBinding(key=pb['key'], value=pb['value']) for pb in conf[\"parameters\"]] \n",
    "  \n",
    "    # Create the configuration with associated parameters and input artifacts\n",
    "    configuration = client.configuration.create(\n",
    "        name=conf[\"name\"],\n",
    "        scenario_id=conf[\"scenario_id\"],\n",
    "        executable_id=conf[\"executable_id\"],\n",
    "        parameter_bindings=parameter_bindings,\n",
    "        input_artifact_bindings = input_artifact_bindings\n",
    "    )\n",
    "    \n",
    "    print(configuration)\n",
    "\n",
    "    # Update the configuration_id in env.json under the corresponding folder\n",
    "    # which will be used in continuos-deployment.ipynb to create deployment automatically.\n",
    "    update_json_file(f'{conf[\"executable_id\"]}/env.json',\"configuration_id\", configuration.id)\n",
    "    config_id = configuration.id\n",
    "\n",
    "# write back to config.json\n",
    "with open(\"config.json\", 'w') as file:\n",
    "        json.dump(config, file, indent=4)\n",
    "        print(\"config.json updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
