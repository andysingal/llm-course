{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-fireworks requests beautifulsoup4"
      ],
      "metadata": {
        "id": "Vxg0M88Zv3KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain_fireworks import ChatFireworks\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "IMHaQ6Qjv13g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"FIREWORKS_API_KEY\"] = getpass(\"üöÄ Enter your Fireworks API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGOohvoTuPPR",
        "outputId": "3e8097bf-5cdc-4607-b3d8-9c407e6848bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Enter your Fireworks API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatFireworks(\n",
        "    model=\"accounts/fireworks/models/llama-v3-70b-instruct\",\n",
        "    temperature=0.6,\n",
        "    max_tokens=1024,\n",
        "    stop=[\"\\n\\n\"]\n",
        ")"
      ],
      "metadata": {
        "id": "H37H5aKHuZn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\n",
        "    {\"role\":\"system\",\"content\":\"You are an expert data-scientist assistant.\"},\n",
        "    {\"role\":\"user\",\"content\":\"Analyze the sentiment of this review:\\n\\n\"\n",
        "                           \"\\\"The new movie was breathtaking, but a bit too long.\\\"\"}\n",
        "]\n",
        "resp = llm.invoke(prompt)\n",
        "print(\"Sentiment Analysis ‚Üí\", resp.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqt24_-QucBq",
        "outputId": "a9d3292d-9011-48cf-843a-17bd112affc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis ‚Üí A great review to analyze!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "You are a data-science assistant. Keep track of the convo:\n",
        "\n",
        "{history}\n",
        "User: {input}\n",
        "Assistant:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"history\",\"input\"], template=template)\n",
        "memory = ConversationBufferMemory(memory_key=\"history\")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
        "\n",
        "print(chain.run(input=\"Hey, what can you do?\"))\n",
        "print(chain.run(input=\"Analyze: 'The product arrived late, but support was helpful.'\"))\n",
        "print(chain.run(input=\"Based on that, would you recommend the service?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT-9Ii29ufkl",
        "outputId": "fb0bfaf4-0879-416c-89e4-323b3d1bf69d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-28d37453855b>:12: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"history\")\n",
            "<ipython-input-10-28d37453855b>:14: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
            "<ipython-input-10-28d37453855b>:16: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  print(chain.run(input=\"Hey, what can you do?\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can assist with a wide range of data science tasks. I can help with data cleaning and preprocessing, exploratory data analysis, visualization, machine learning modeling, and more. I can also provide guidance on data science concepts and best practices.\n",
            "I'd be happy to analyze the given text.\n",
            "Based on the analysis, I would say that the recommendation is neutral or mixed. The customer mentions a negative aspect (the product arrived late), but also a positive aspect (the support was helpful). This suggests that the service has some room for improvement, but also has some redeeming qualities. Overall, it's not a strongly positive or negative review.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FetchURLTool(BaseTool):\n",
        "    name: str = \"fetch_url\"\n",
        "    description: str = \"Fetch the main text (first 500 chars) from a webpage.\"\n",
        "\n",
        "    def _run(self, url: str) -> str:\n",
        "        resp = requests.get(url, timeout=10)\n",
        "        doc = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        paras = [p.get_text() for p in doc.find_all(\"p\")][:5]\n",
        "        return \"\\n\\n\".join(paras)\n",
        "\n",
        "    async def _arun(self, url: str) -> str:\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "sXFxXScZwV75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerateSQLTool(BaseTool):\n",
        "    name: str = \"generate_sql\"\n",
        "    description: str = \"Generate a BigQuery SQL query (with comments) from a text description.\"\n",
        "\n",
        "    def _run(self, text: str) -> str:\n",
        "        prompt = f\"\"\"\n",
        "-- Requirement:\n",
        "-- {text}\n",
        "\n",
        "-- Write a BigQuery SQL query (with comments) to satisfy the above.\n",
        "\"\"\"\n",
        "        return llm.invoke([{\"role\":\"user\",\"content\":prompt}]).content\n",
        "\n",
        "    async def _arun(self, text: str) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "tools = [FetchURLTool(), GenerateSQLTool()]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "result = agent.run(\n",
        "    \"Fetch https://en.wikipedia.org/wiki/ChatGPT \"\n",
        "    \"and then generate a BigQuery SQL query that counts how many times \"\n",
        "    \"the word 'model' appears in the page text.\"\n",
        ")\n",
        "\n",
        "print(\"\\nüîç Generated SQL:\\n\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7K0XP7RzwTP",
        "outputId": "bf568cd7-15ae-4f2d-b4ad-f8a7079c397c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to fetch the webpage content first to get the text.\n",
            "\n",
            "Action: fetch_url\n",
            "Action Input: https://en.wikipedia.org/wiki/ChatGPT\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\n",
            "\n",
            "\n",
            "ChatGPT is a generative artificial intelligence chatbot developed by the American company OpenAI and launched in 2022. It is based on large language models (LLMs) such as GPT-4o. ChatGPT can generate human-like conversational responses and enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language.[2] It is credited with accelerating the AI boom, which has led to ongoing rapid investment in and public attention to the field of artificial intelligence (AI).[3] Some observers have raised concern about the potential of ChatGPT and similar programs to displace human intelligence, enable plagiarism, or fuel misinformation.[4][5]\n",
            "\n",
            "\n",
            "ChatGPT is built on OpenAI's proprietary series of generative pre-trained transformer (GPT) models and is fine-tuned for conversational applications using a combination of supervised learning and reinforcement learning from human feedback.[4] Successive user prompts and replies are considered at each conversation stage as context.[6] ChatGPT was released as a freely available research preview, but due to its popularity, OpenAI now operates the service on a freemium model. Users on its free tier can access GPT-4o. The ChatGPT \"Plus\", \"Pro\", \"Team\", and \"Enterprise\" subscriptions provide additional features such as DALL-E 3 image generation, more capable AI models, and an increased usage limit.[7]\n",
            "\n",
            "\n",
            "By January 2023, ChatGPT had become what was then the fastest-growing consumer software application in history, gaining over 102 million users in two months.[8][9] ChatGPT's release spurred the release of competing products, including Gemini, Claude, Llama, Ernie, and Grok.[10] Microsoft launched Copilot, initially based on OpenAI's GPT-4. In May 2024, a partnership between Apple Inc. and OpenAI was announced, in which ChatGPT was integrated into the Apple Intelligence feature of Apple operating systems.[11] As of April 2025, ChatGPT's website is among the 10 most-visited websites globally.[12][13]\n",
            "\n",
            "\n",
            "ChatGPT is based on GPT foundation models that were fine-tuned for conversational assistance, including GPT-4o, GPT-4.5, o3, and o4-mini.[14] The fine-tuning process leveraged supervised learning and reinforcement learning from human feedback (RLHF).[15][16] Both approaches employed human trainers to improve model performance. In the case of supervised learning, the trainers played both sides: the user and the AI assistant. In the reinforcement learning stage, human trainers first ranked responses that the model had created in a previous conversation.[17] These rankings were used to create \"reward models\" that were used to fine-tune the model further by using several iterations of proximal policy optimization.[15][18]\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have the text of the webpage. Now I need to generate a BigQuery SQL query that counts how many times the word 'model' appears in the page text.\n",
            "\n",
            "Action: generate_sql\n",
            "Action Input: Count the occurrences of the word 'model' in a given text.\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mHere is a BigQuery SQL query that counts the occurrences of the word 'model' in a given text:\n",
            "```\n",
            "-- Assuming the text is stored in a column named `text` in a table named `my_table`\n",
            "WITH \n",
            "  -- Split the text into individual words\n",
            "  words AS (\n",
            "    SELECT \n",
            "      SPLIT(text, ' ') AS word_array\n",
            "    FROM \n",
            "      my_table\n",
            "  ),\n",
            "  \n",
            "  -- Count the occurrences of the word 'model'\n",
            "  model_counts AS (\n",
            "    SELECT \n",
            "      COUNTIF(word = 'model') AS model_count\n",
            "    FROM \n",
            "      words, \n",
            "      UNNEST(word_array) AS word\n",
            "  )\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
            "\n",
            "Final Answer: The BigQuery SQL query to count the occurrences of the word 'model' in the page text is:\n",
            "```\n",
            "WITH \n",
            "  -- Split the text into individual words\n",
            "  words AS (\n",
            "    SELECT \n",
            "      SPLIT(text, ' ') AS word_array\n",
            "    FROM \n",
            "      my_table\n",
            "  ),\n",
            "  \n",
            "  -- Count the occurrences of the word 'model'\n",
            "  model_counts AS (\n",
            "    SELECT \n",
            "      COUNTIF(word = 'model') AS model_count\n",
            "    FROM \n",
            "      words, \n",
            "      UNNEST(word_array) AS word\n",
            "  )\n",
            "```\n",
            "Note: This query assumes that the text is stored in a column named `text` in a table named `my_table`. You would need to replace `my_table` with the actual table name and schema in your BigQuery dataset.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "üîç Generated SQL:\n",
            " The BigQuery SQL query to count the occurrences of the word 'model' in the page text is:\n",
            "```\n",
            "WITH \n",
            "  -- Split the text into individual words\n",
            "  words AS (\n",
            "    SELECT \n",
            "      SPLIT(text, ' ') AS word_array\n",
            "    FROM \n",
            "      my_table\n",
            "  ),\n",
            "  \n",
            "  -- Count the occurrences of the word 'model'\n",
            "  model_counts AS (\n",
            "    SELECT \n",
            "      COUNTIF(word = 'model') AS model_count\n",
            "    FROM \n",
            "      words, \n",
            "      UNNEST(word_array) AS word\n",
            "  )\n",
            "```\n",
            "Note: This query assumes that the text is stored in a column named `text` in a table named `my_table`. You would need to replace `my_table` with the actual table name and schema in your BigQuery dataset.\n"
          ]
        }
      ]
    }
  ]
}