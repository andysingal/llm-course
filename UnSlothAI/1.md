# Unsloth
Unsloth focuses on several optimizations:

1. Intelligent Weight Upcasting: To fine-tune with QLoRA, specific layers like the language modeling head are upcasted to float32, enhancing stability during fine-tuning as only float16 weights can be unstable. Fewer upcasted weights result in lower memory usage and faster fine-tuning.

2. Leveraging bfloat16: Unsloth effectively utilizes bfloat16, a superior data type for fine-tuning processes, optimizing operations.

3. xFormers Framework Integration: Utilizes xFormers to optimize transformer model building blocks through custom Triton kernels. Defaultly employs Flash-Attention 2, enhancing acceleration.

4. Causal LLM Optimization: Implements causal attention masks to restrict attention during fine-tuning to previous positions, speeding up the process.

5. RoPE Implementation with Triton: Utilizes Triton to implement Relative Positional Encodings (RoPE), further accelerating models.

Moreover, OpenAI's Triton facilitates reaching peak hardware performance with minimal programming effort. For instance, it enables creating FP16 matrix multiplication kernels, akin to cuBLAS performance, in under 25 lines of code.

Lastly, Unsloth employs Triton for implementing RMSNorm, a simpler and more efficient alternative to LayerNorm. RMSNorm offers acceleration to Language Learning Models (LLMs), imparts re-scaling invariance, and facilitates implicit adaptation to learning rates.


## Resources:
- https://github.com/unslothai/unsloth
- [Finetuning with Unsloth](https://mer.vin/2024/02/unsloth-fine-tuning/)
- [4x longer context windows & 1.7x larger batch sizes](https://unsloth.ai/blog/long-context)


## Example:
- [Fine-Tuning Llama 3.2 Vision for Accurate Calorie Extraction from Food Images](https://www.analyticsvidhya.com/blog/2025/02/fine-tuning-llama-3-2-vision/)
- [Reason-Model](https://x.com/BrianRoemmele/status/1894521264511098898)


