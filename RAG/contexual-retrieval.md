What is ğ—–ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜ğ˜‚ğ—®ğ—¹ ğ—¥ğ—²ğ˜ğ—¿ğ—¶ğ—²ğ˜ƒğ—®ğ—¹ and why might it be important for your AI applications?

The idea of Contextual Retrieval was suggested by the Anthropic team late last year. It aims to improve accuracy and relevance of data that is retrieved in Retrieval Augmented Generation based AI systems.

There have been various methods suggested for improving the process before:

â¡ï¸ Overlapping chunking.
â¡ï¸ Hierarchical chunking.
â¡ï¸ â€¦

I personally tried at least a few with different levels of success (they do help). 

However, I love the intuitiveness and simplicity of Contextual Retrieval. And it does provide better results.

ğ˜—ğ˜³ğ˜¦ğ˜±ğ˜³ğ˜°ğ˜¤ğ˜¦ğ˜´ğ˜´ğ˜ªğ˜¯ğ˜¨:

ğŸ­. Split each of your documents into chunks via chosen chunking strategy. 
ğŸ®. For each chunk separately, add it to a prompt together with the whole document. 
ğŸ¯. Include instructions to situate the chunk in the document and generate short context for it. Pass the prompt to a chosen LLM.
ğŸ°. Combine the context that was generated in the previous step and the chunk that the context was generated for.
ğŸ±. Pass the data through a TF-IDF embedder.
ğŸ². Pass the data through a LLM based embedding model.
ğŸ³. Store the data generated in steps 5. and 6. in databases that support efficient search.

ğ˜™ğ˜¦ğ˜µğ˜³ğ˜ªğ˜¦ğ˜·ğ˜¢ğ˜­:

ğŸ´. Use user query for relevant context retrieval. ANN search for semantic and TF-IDF index for exact search.
ğŸµ. Use Rank Fusion techniques to combine and deduplicate the retrieved results and produce top N elements.
ğŸ­ğŸ¬. Rerank the previous results and narrow down to top K elements.
ğŸ­ğŸ­. Pass the result of step 10. to a LLM together with the user query to produce the final answer.

â—ï¸ Step 3. might sound extremely costly and it is, but with Prompt Caching, the costs can be significantly reduced.
âœ… Prompt caching can be implemented in both proprietary and open source model cases (remember Cache Augmented Generation?).
