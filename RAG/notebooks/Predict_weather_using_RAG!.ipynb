{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s58-d0EMxwz2"
      },
      "source": [
        "# Predicting weather in the next hour using raw data\n",
        "\n",
        "This notebook demonstrates how we can use vector search for time series forecasting on climate data with Pinecone.\n",
        "We use the [Jena Climate dataset](https://www.kaggle.com/stytch16/jena-climate-2009-2016) for this example. Jena Climate dataset is made up of quantities such as air temperature, atmospheric pressure, humidity, wind direction, etc. that were recorded every 10 minutes, over several years.\n",
        "\n",
        "In a tabular dataset like this, every column can be seen as a feature vector identified uniquely by the time stamp associated with them. We can use these vectors to perform similarity search with a given query vector at a certain time to predict the weather for that hour. Though a very simple embedding extraction process, we want to see how far we can get even with a basic similarity search method like this. We will see how to do with Pinecone in the steps below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdAK--RKzbBg"
      },
      "source": [
        "### Install Pinecone\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84AaX9pJre4p"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    pinecone-client==3.1.0 \\\n",
        "    matplotlib==3.2.2 \\\n",
        "    tensorflow==2.9.2 \\\n",
        "    scikit-learn==1.0.2 \\\n",
        "    pandas==1.3.5 \\\n",
        "    tqdm\\\n",
        "    pinecone-notebooks==0.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zkYAjcF37k4"
      },
      "source": [
        "You can get your Pinecone API Key [here](https://www.pinecone.io/start/) if you don't have one."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# initialize connection to pinecone (orget API key at app.pinecone.io)\n",
        "if not os.environ.get(\"PINECONE_API_KEY\"):\n",
        "    from pinecone_notebooks.colab import Authenticate\n",
        "    Authenticate()"
      ],
      "metadata": {
        "id": "X_jBq5s84Dvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZqWpleX3fLB"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n",
        "\n",
        "# configure client\n",
        "pc = Pinecone(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKkBILpd3fLB"
      },
      "source": [
        "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbfy7C0x3fLB"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR-PpDoD1PG0"
      },
      "source": [
        "### Import other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MneIXaY6RYdG"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List\n",
        "import itertools\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (20, 16)\n",
        "mpl.rcParams['axes.grid'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIWyGBUbx84O"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkzmGFhhRqgi"
      },
      "outputs": [],
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3UCCSbGy2MR"
      },
      "source": [
        "Load the hourly data into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2TMTpT2WaUc"
      },
      "outputs": [],
      "source": [
        "original_data_for_insert = pd.read_csv(csv_path)\n",
        "original_data_for_insert = original_data_for_insert[5::6]\n",
        "\n",
        "original_data_for_insert['Date Time'] = pd.to_datetime(original_data_for_insert['Date Time'], format='%d.%m.%Y %H:%M:%S')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02P8QFVTy4ic"
      },
      "source": [
        "Split data into data that is going to be inserted into Pinecone, and data that is going to be used for querying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q8MtzYkW3N-"
      },
      "outputs": [],
      "source": [
        "n = len(original_data_for_insert)\n",
        "train_data = original_data_for_insert[:int(n*0.9)]\n",
        "test_data = original_data_for_insert[int(n*0.9):]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tws_z661y-98"
      },
      "source": [
        "\n",
        "Let's see what the data looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkY531VLYW3p"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkxzp4M0zB0b"
      },
      "source": [
        "Prepare data for upload. We will be querying data by the date and time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rByDWqoxXH9C"
      },
      "outputs": [],
      "source": [
        "items_to_upload = []\n",
        "for row in train_data.values.tolist():\n",
        "    key = str(row[0])\n",
        "    values = row[1:]\n",
        "    items_to_upload.append((key, values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0r_v3YszIZE"
      },
      "source": [
        "Prepare data that is going to be queried.\n",
        "Here we create two lists - one with dates that are going to be queried and the other one with vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2m0keJsZ_LP"
      },
      "outputs": [],
      "source": [
        "query_dates = []\n",
        "query_data = []\n",
        "for row in test_data.values.tolist():\n",
        "    query_dates.append(str(row[0]))\n",
        "    query_data.append(row[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYvbDcSQEFXA"
      },
      "source": [
        "### Setting up an index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2BASl-QsATL"
      },
      "outputs": [],
      "source": [
        "# Pick a name for the new service\n",
        "index_name = 'time-series-weather'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYtIJqJ63fLE"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=14,\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTZe8ieTsYOl"
      },
      "outputs": [],
      "source": [
        "# Upload items\n",
        "def chunks(iterable, batch_size=100):\n",
        "    it = iter(iterable)\n",
        "    chunk = tuple(itertools.islice(it, batch_size))\n",
        "    while chunk:\n",
        "        yield chunk\n",
        "        chunk = tuple(itertools.islice(it, batch_size))\n",
        "\n",
        "for batch in chunks(items_to_upload, 500):\n",
        "    index.upsert(vectors=batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zMX2j2PsY34"
      },
      "outputs": [],
      "source": [
        "# Check the index size to confirm the data was upserted properly\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1erBaBGvvRt"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Query items\n",
        "all_query_results = []\n",
        "for xq in tqdm(query_data):\n",
        "    res = index.query(vector=xq, top_k=1)\n",
        "    all_query_results.append(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IaWa15tzjv1"
      },
      "source": [
        "Here we create a function for getting predictions from Pinecone. We do this by using vectors to find the most similar vector in the index and then reading the hour after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3JYRygzB03y"
      },
      "outputs": [],
      "source": [
        "def get_predictions(feature: str) -> (List, List):\n",
        "\n",
        "    true_values = []\n",
        "    predicted_values = []\n",
        "\n",
        "    for test_date, qr in zip(query_dates, all_query_results):\n",
        "        similar_date = [res.id for res in qr.matches][0]\n",
        "        hour_from_original = datetime.strptime(str(test_date), '%Y-%m-%d %H:%M:%S') + timedelta(hours=1)\n",
        "        hour_from_similar = datetime.strptime(similar_date, '%Y-%m-%d %H:%M:%S') + timedelta(hours=1)\n",
        "\n",
        "        original_temperature = original_data_for_insert.loc[original_data_for_insert['Date Time'] == hour_from_original][feature].tolist()\n",
        "        similar_temperature = original_data_for_insert.loc[original_data_for_insert['Date Time'] == hour_from_similar][feature].tolist()\n",
        "\n",
        "        if original_temperature and similar_temperature:\n",
        "            true_values.append(original_temperature[0])\n",
        "            predicted_values.append(similar_temperature[0])\n",
        "    return true_values, predicted_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYMfkelz-cdS"
      },
      "outputs": [],
      "source": [
        "def plot_results(predicted_values: List, true_values: List):\n",
        "    x_list = range(0, len(predicted_values))\n",
        "    plt.plot(x_list[:200], predicted_values[:200], label='forecast')\n",
        "    plt.plot(x_list[:200], true_values[:200], label='true')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9VqLf2Hsz_X"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def print_results(true_values: List, predicted_values: List):\n",
        "    print(f'MSE: {mean_squared_error(true_values, predicted_values)}')\n",
        "    print(f'RMSE: {mean_squared_error(true_values, predicted_values, squared=False)}')\n",
        "    print(f'MAE: {mean_absolute_error(true_values, predicted_values)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yngMLgyz1ru"
      },
      "source": [
        "### Results\n",
        "\n",
        "To evaluate our results we will plot the predicted and true values for all the 14 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKwJdDq4usIt"
      },
      "outputs": [],
      "source": [
        "for feature in original_data_for_insert.columns[1:]:\n",
        "    print(f'Analyzing predictions for {feature}')\n",
        "    true_values, predicted_values = get_predictions(feature)\n",
        "    plot_results(true_values, predicted_values)\n",
        "    print_results(true_values, predicted_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v87PO9maGZn3"
      },
      "source": [
        "### Summary\n",
        "\n",
        "From the plots above we can see that the method is able to predict pretty accurately for feature like VPdef, VPmax, rh(%) etc. predict roughly accurately for features like H20C, rho and is not that great for features like wd, max.vv, wv. Given how simple the approach is and doesn't involve any feature engineering, it does pretty good in some spots!\n",
        "\n",
        "We can improve these predictions by using more complex methods like LSTMs which are better suited to handle data like these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f04emmGU_iWD"
      },
      "source": [
        "### Delete the Index\n",
        "\n",
        "Once we don't have use of the index we can delete them.\n",
        "\n",
        "\n",
        "*Note: Index deletion is permanent*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCN2URTDARUT"
      },
      "outputs": [],
      "source": [
        "pc.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "glaN13904tLz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}